"""
LightZero ç¯å¢ƒåŒ…è£…å™¨
å°† CircuitDesignerDiscrete é€‚é…ä¸º LightZero çš„ BaseEnv æ¥å£
"""

import os
import time
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from easydict import EasyDict
from ding.envs.env.base_env import BaseEnv, BaseEnvTimestep
from ding.utils.registry_factory import ENV_REGISTRY
from gymnasium import spaces

from .circuit_sys_discrete import CircuitDesignerDiscrete


@ENV_REGISTRY.register('circuit_designer_lightzero')
class CircuitDesignerLightZeroEnv(BaseEnv):
    """
    LightZero ç¯å¢ƒåŒ…è£…å™¨ï¼Œç”¨äºé‡å­ç”µè·¯è®¾è®¡ç¯å¢ƒ
    """
    
    def __init__(self, cfg: EasyDict = None):
        """
        åˆå§‹åŒ–ç¯å¢ƒ
        
        Args:
            cfg: ç¯å¢ƒé…ç½®å­—å…¸ï¼ŒåŒ…å«:
                - max_qubits: æœ€å¤§é‡å­æ¯”ç‰¹æ•°
                - max_gates: æœ€å¤§é—¨æ•°é‡
                - objective: ç›®æ ‡ä»»åŠ¡ï¼ˆå¦‚'SP-bell'ï¼‰
                - punish: æ˜¯å¦ä½¿ç”¨æƒ©ç½šé¡¹
                - fidelity_threshold: ä¿çœŸåº¦é˜ˆå€¼
                - save_dir: episodeæ•°æ®ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
                - enable_monitor: æ˜¯å¦å¯ç”¨episodeç›‘æ§ï¼ˆé»˜è®¤Trueï¼‰
                - env_id: ç¯å¢ƒIDï¼ˆç”¨äºå¤šè¿›ç¨‹æ—¶åŒºåˆ†ä¸åŒç¯å¢ƒï¼‰
        """
        if cfg is None:
            cfg = EasyDict({
                'max_qubits': 2,
                'max_gates': 5,
                'objective': 'SP-bell',
                'punish': False,
                'fidelity_threshold': 0.99,
                'enable_monitor': True
            })
        
        # åˆ›å»ºåŸå§‹ç¯å¢ƒ
        self.env = CircuitDesignerDiscrete(
            max_qubits=cfg.max_qubits,
            max_gates=cfg.max_gates,
            objective=cfg.objective,
            punish=cfg.punish,
            fidelity_threshold=cfg.fidelity_threshold
        )
        
        # ä¿å­˜é…ç½®
        self.cfg = cfg
        
        # ğŸ”¥ å¯ç”¨ç¯å¢ƒå†…ç½®è®°å½•ï¼ˆæ›´å¯é ï¼ï¼‰
        self.enable_monitor = cfg.get('enable_monitor', True)
        self.save_dir = cfg.get('save_dir', None)
        
        # ğŸ”¥ å¦‚æœæ²¡æœ‰save_dirï¼Œä½†å¯ç”¨äº†monitorï¼Œè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªé»˜è®¤ç›®å½•
        if self.enable_monitor and self.save_dir is None:
            # ä»objectiveç”Ÿæˆé»˜è®¤ç›®å½•å
            task_name = cfg.objective.replace('-', '_')
            self.save_dir = f'results/AlphaZero_{task_name}_q{cfg.max_qubits}_g{cfg.max_gates}/episodes'
        
        # ä½¿ç”¨è¿›ç¨‹ID+æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€çš„env_id
        self.env_id = cfg.get('env_id', f"{os.getpid()}_{int(time.time()*1000000) % 1000000}")
        
        # Episodeæ•°æ®ç¼“å­˜
        self._episode_data = []
        self._episode_count = 0
        self._current_episode_rewards = []
        self._current_episode_actions = []
        
        # è½¨è¿¹è®°å½•
        self._current_trajectory = {
            'step_rewards': [],
            'step_fidelities': [],
            'step_actions': [],
            'step_gate_counts': [],
            'step_circuit_depths': []
        }
        
        # ğŸ”¥ è®­ç»ƒè¿›åº¦è¿½è¸ªï¼ˆç”¨äºtraining_progress.csvï¼‰
        self._total_timesteps = 0  # æ€»ç¯å¢ƒäº¤äº’æ­¥æ•°
        self._progress_window = []  # æœ€è¿‘100ä¸ªepisodesçš„ç»Ÿè®¡
        self._last_progress_timesteps = 0  # ä¸Šæ¬¡ä¿å­˜è¿›åº¦çš„timesteps
        self._progress_interval = 10000  # ğŸ”¥ æ¯200æ­¥ä¿å­˜ä¸€æ¬¡è¿›åº¦ï¼ˆä¾¿äºæµ‹è¯•ï¼‰
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        if self.enable_monitor and self.save_dir:
            self._init_episode_files()
        
        # AlphaZero å…¼å®¹å±æ€§
        self.battle_mode = 'self_play_mode'
        self.battle_mode_in_simulation_env = 'self_play_mode'
        self.current_player = 0  # å•ç©å®¶æ¸¸æˆ
        
        # å½“å‰çŠ¶æ€ç¼“å­˜
        self._current_obs = None
        
        # ç´¯ç§¯å¥–åŠ±ï¼ˆç”¨äºeval_episode_returnï¼‰
        self._episode_reward = 0
        
        # éšæœºç§å­
        self._seed = None
        self._dynamic_seed = True
        
    @property
    def observation_space(self):
        """è§‚å¯Ÿç©ºé—´"""
        return self.env.observation_space
    
    @property
    def action_space(self):
        """åŠ¨ä½œç©ºé—´"""
        return self.env.action_space
    
    @property
    def reward_space(self):
        """å¥–åŠ±ç©ºé—´"""
        return spaces.Box(
            low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32
        )
    
    @property
    def legal_actions(self):
        """
        è¿”å›å½“å‰åˆæ³•çš„åŠ¨ä½œåˆ—è¡¨
        å¯¹äºé‡å­ç”µè·¯ç¯å¢ƒï¼Œæ‰€æœ‰åŠ¨ä½œéƒ½æ˜¯åˆæ³•çš„
        """
        return list(range(self.action_space.n))
    
    def current_state(self):
        """
        è¿”å›å½“å‰çŠ¶æ€ï¼ˆç”¨äºMCTSæ¨¡æ‹Ÿï¼‰
        
        Returns:
            tuple: (åŸå§‹çŠ¶æ€, å½’ä¸€åŒ–çŠ¶æ€)
        """
        if self._current_obs is None:
            obs, _ = self.env._state_current_state()
            self._current_obs = obs
        return (self._current_obs, self._current_obs)
    
    def get_done_winner(self):
        """
        æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸåŠè·èƒœè€…
        å¯¹äºå•ç©å®¶ä¼˜åŒ–ä»»åŠ¡ï¼Œ"è·èƒœ"æ„å‘³ç€è¾¾åˆ°ç›®æ ‡ä¿çœŸåº¦
        
        Returns:
            tuple: (æ˜¯å¦ç»“æŸ, è·èƒœè€…åˆ—è¡¨)
                - æˆåŠŸè¾¾åˆ°ç›®æ ‡: (True, [0])
                - è¾¾åˆ°æœ€å¤§é™åˆ¶ä½†æœªæˆåŠŸ: (True, [-1])
                - æœªå®Œæˆ: (False, [-1])
        """
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if hasattr(self.env, 'reached') and self.env.reached:
            return True, [0]  # è¾¾åˆ°ç›®æ ‡ï¼Œç©å®¶0è·èƒœ
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é—¨æ•°ï¼ˆå¤±è´¥ï¼‰
        if len(self.env._qc.data) >= self.env.max_gates:
            return True, [-1]  # è¾¾åˆ°æœ€å¤§é—¨æ•°ä½†æœªæˆåŠŸï¼Œæ¸¸æˆç»“æŸä½†å¤±è´¥
        
        return False, [-1]  # æ¸¸æˆæœªå®Œæˆ
    
    def reset(self, start_player_index=0, init_state=None, **kwargs):
        """
        é‡ç½®ç¯å¢ƒ
        
        Args:
            start_player_index: èµ·å§‹ç©å®¶ç´¢å¼•ï¼ˆå•ç©å®¶æ¸¸æˆå¿½ç•¥ï¼‰
            init_state: åˆå§‹çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            dict: è§‚å¯Ÿå­—å…¸ï¼ŒåŒ…å«:
                - observation: åŸå§‹è§‚å¯Ÿ
                - board: AlphaZeroæœŸæœ›çš„æ£‹ç›˜çŠ¶æ€
                - current_player_index: å½“å‰ç©å®¶ç´¢å¼•
                - action_mask: åŠ¨ä½œæ©ç ï¼ˆæ‰€æœ‰åŠ¨ä½œå¯ç”¨ï¼‰
                - to_play: è½®åˆ°è°ä¸‹ï¼ˆå•ç©å®¶å›ºå®šä¸º0ï¼‰
        """
        obs, info = self.env.reset()
        self._current_obs = obs
        self._episode_reward = 0  # é‡ç½®ç´¯ç§¯å¥–åŠ±
        self._current_episode_rewards = []  # é‡ç½®episodeå¥–åŠ±åˆ—è¡¨
        self._current_episode_actions = []  # é‡ç½®episodeåŠ¨ä½œåˆ—è¡¨
        
        # ğŸ”¥ é‡ç½®è½¨è¿¹è®°å½•
        self._current_trajectory = {
            'step_rewards': [],
            'step_fidelities': [],
            'step_actions': [],
            'step_gate_counts': [],
            'step_circuit_depths': []
        }
        
        # åˆ›å»ºåŠ¨ä½œæ©ç ï¼ˆæ‰€æœ‰åŠ¨ä½œéƒ½å¯ç”¨ï¼‰
        action_mask = np.ones(self.action_space.n, dtype=np.int8)
        
        # è¿”å› AlphaZero æœŸæœ›çš„æ ¼å¼ï¼ˆåªè¿”å›obså­—å…¸ï¼Œä¸è¿”å›infoï¼‰
        obs_dict = {
            'observation': obs,
            'board': obs,  # AlphaZeroæœŸæœ›çš„å­—æ®µ
            'current_player_index': 0,  # å•ç©å®¶æ¸¸æˆ
            'action_mask': action_mask,
            'to_play': 0  # å•ç©å®¶æ¸¸æˆå›ºå®šä¸º0
        }
        
        return obs_dict
    
    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ
        
        Args:
            action: åŠ¨ä½œID
            
        Returns:
            BaseEnvTimestep: åŒ…å«obs, reward, done, infoçš„æ—¶é—´æ­¥
        """
        obs, reward, terminated, truncated, info = self.env.step(action)
        self._current_obs = obs
        
        # ğŸ”¥ è¿½è¸ªæ€»timesteps
        self._total_timesteps += 1
        
        # è®°å½•å½“å‰æ­¥çš„æ•°æ®
        self._current_episode_rewards.append(reward)
        self._current_episode_actions.append(action)
        
        # ğŸ”¥ è®°å½•è½¨è¿¹ä¿¡æ¯ï¼ˆæ¯ä¸€æ­¥ï¼‰
        self._current_trajectory['step_rewards'].append(float(reward))
        self._current_trajectory['step_fidelities'].append(float(info.get('fidelity', 0.0)))
        self._current_trajectory['step_actions'].append(int(action))
        self._current_trajectory['step_gate_counts'].append(int(info.get('gate_count', 0)))
        self._current_trajectory['step_circuit_depths'].append(int(info.get('circuit_depth', 0)))
        
        # åˆ›å»ºåŠ¨ä½œæ©ç 
        action_mask = np.ones(self.action_space.n, dtype=np.int8)
        
        # åˆ›å»ºè§‚å¯Ÿå­—å…¸
        obs_dict = {
            'observation': obs,
            'board': obs,
            'current_player_index': 0,
            'action_mask': action_mask,
            'to_play': 0  # å•ç©å®¶æ¸¸æˆå›ºå®šä¸º0
        }
        
        # ç´¯ç§¯å¥–åŠ±
        self._episode_reward += reward
        
        # LightZero ä½¿ç”¨ terminated è¡¨ç¤ºæ¸¸æˆç»“æŸ
        done = terminated or truncated
        
        # Episodeç»“æŸæ—¶è®°å½•æ•°æ®
        if done:
            info['eval_episode_return'] = self._episode_reward
            
            # ğŸ”¥ ä»ç¯å¢ƒçš„infoç›´æ¥æå–çœŸå®æ•°æ®
            final_fidelity = info.get('fidelity', 0.0)
            gate_count = info.get('gate_count', 0)
            episode_length = len(self._current_episode_actions)
            success = final_fidelity >= self.cfg.fidelity_threshold
            
            # ğŸ”¥ æ·»åŠ åˆ°è¿›åº¦çª—å£ï¼ˆç”¨äºtraining_progress.csvï¼‰
            self._progress_window.append({
                'total_reward': float(self._episode_reward),
                'fidelity': float(final_fidelity),
                'length': int(gate_count),
                'success': float(success)
            })
            
            # ä¿æŒçª—å£å¤§å°ä¸º100
            if len(self._progress_window) > 100:
                self._progress_window.pop(0)
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜è®­ç»ƒè¿›åº¦
            if self._total_timesteps - self._last_progress_timesteps >= self._progress_interval:
                self._save_training_progress()
                self._last_progress_timesteps = self._total_timesteps
            
            # è®°å½•episodeæ•°æ®ï¼ˆåŒ…å«è½¨è¿¹ï¼‰
            self._record_episode(
                total_reward=self._episode_reward,
                length=episode_length,
                final_fidelity=final_fidelity,
                gate_count=gate_count,
                success=success,
                trajectory=self._current_trajectory.copy()  # ä¼ é€’è½¨è¿¹å‰¯æœ¬
            )
            
            # æ¸…ç©ºå½“å‰episodeçš„ç¼“å­˜
            self._current_episode_rewards = []
            self._current_episode_actions = []
            # é‡ç½®è½¨è¿¹
            self._current_trajectory = {
                'step_rewards': [],
                'step_fidelities': [],
                'step_actions': [],
                'step_gate_counts': [],
                'step_circuit_depths': []
            }
        
        # è¿”å›BaseEnvTimestep
        return BaseEnvTimestep(obs_dict, reward, done, info)
    
    def _init_episode_files(self):
        """åˆå§‹åŒ–episodeè®°å½•æ–‡ä»¶"""
        Path(self.save_dir).mkdir(parents=True, exist_ok=True)
        
        # CSVæ–‡ä»¶è·¯å¾„ï¼ˆæ¯ä¸ªç¯å¢ƒæœ‰ç‹¬ç«‹çš„ä¸´æ—¶æ–‡ä»¶ï¼‰
        self.csv_path = os.path.join(self.save_dir, f'episodes_env{self.env_id}.csv')
        
        # ğŸ”¥ JSONLæ–‡ä»¶è·¯å¾„ï¼ˆè¯¦ç»†è½¨è¿¹ï¼‰
        self.jsonl_path = os.path.join(self.save_dir, f'episodes_detailed_env{self.env_id}.jsonl')
        
        # ğŸ”¥ è®­ç»ƒè¿›åº¦æ–‡ä»¶è·¯å¾„ï¼ˆæ‰€æœ‰ç¯å¢ƒå…±äº«ï¼Œè¿½åŠ æ¨¡å¼ï¼‰
        self.progress_path = os.path.join(self.save_dir, 'training_progress.csv')
        
        # ğŸ”¥ ä½¿ç”¨ä¸DQNç›¸åŒçš„CSVæ ¼å¼
        if not os.path.exists(self.csv_path):
            df = pd.DataFrame(columns=[
                'episode', 'total_reward', 'final_fidelity', 'length', 'success'
            ])
            df.to_csv(self.csv_path, index=False)
        
        # åˆå§‹åŒ–training_progress.csvï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        if not os.path.exists(self.progress_path):
            df_progress = pd.DataFrame(columns=[
                'timesteps', 'mean_reward', 'std_reward', 'mean_fidelity', 
                'std_fidelity', 'success_rate', 'mean_length'
            ])
            df_progress.to_csv(self.progress_path, index=False)
    
    def _record_episode(self, total_reward, length, final_fidelity, gate_count, success, trajectory=None):
        """è®°å½•ä¸€ä¸ªepisodeçš„æ•°æ®"""
        if not self.enable_monitor or not self.save_dir:
            return
        
        self._episode_count += 1
        
        # ğŸ”¥ ä½¿ç”¨ä¸DQNå®Œå…¨ç›¸åŒçš„æ ¼å¼
        # DQNæ ¼å¼ï¼šepisode, total_reward, length(å®é™…æ˜¯gate_count), fidelity, success
        episode_data = {
            'episode': self._episode_count,
            'total_reward': float(total_reward),
            'final_fidelity': int(gate_count),  # ğŸ”¥ ç¬¬3åˆ—å®é™…æ˜¯gate_countï¼
            'length': float(final_fidelity),    # ğŸ”¥ ç¬¬4åˆ—å®é™…æ˜¯fidelityï¼
            'success': bool(success),
        }
        
        self._episode_data.append(episode_data)
        
        # ğŸ”¥ ä¿å­˜è¯¦ç»†è½¨è¿¹åˆ°JSONL
        if trajectory is not None:
            self._save_trajectory(
                episode=self._episode_count,
                total_reward=total_reward,
                length=length,
                final_fidelity=final_fidelity,
                success=success,
                trajectory=trajectory
            )
        
        # æ¯100ä¸ªepisodesä¿å­˜ä¸€æ¬¡
        if len(self._episode_data) >= 100:
            self._save_episodes()
            self._trigger_merge()  # ğŸ”¥ è§¦å‘CSVåˆå¹¶
    
    def _save_episodes(self):
        """ä¿å­˜ç´¯ç§¯çš„episodeæ•°æ®åˆ°CSV"""
        if len(self._episode_data) == 0:
            return
        
        df = pd.DataFrame(self._episode_data)
        df.to_csv(self.csv_path, mode='a', header=False, index=False)
        self._episode_data = []
    
    def _save_trajectory(self, episode, total_reward, length, final_fidelity, success, trajectory):
        """ä¿å­˜è¯¦ç»†è½¨è¿¹åˆ°JSONLæ–‡ä»¶"""
        import json
        
        # æ„é€ è½¨è¿¹è®°å½•ï¼ˆä¸DQNæ ¼å¼ä¸€è‡´ï¼Œæ·»åŠ env_idæ ‡è¯†ï¼‰
        trajectory_record = {
            'env_id': self.env_id,  # ğŸ”¥ æ ‡è¯†æ¥è‡ªå“ªä¸ªç¯å¢ƒ
            'episode': episode,
            'total_reward': float(total_reward),
            'length': int(length),
            'final_fidelity': float(final_fidelity),
            'success': bool(success),
            'trajectory': trajectory
        }
        
        # è¿½åŠ åˆ°JSONLæ–‡ä»¶
        with open(self.jsonl_path, 'a') as f:
            f.write(json.dumps(trajectory_record) + '\n')
    
    def _save_training_progress(self):
        """ä¿å­˜è®­ç»ƒè¿›åº¦ç»Ÿè®¡åˆ°training_progress.csv"""
        if len(self._progress_window) == 0:
            return
        
        import numpy as np
        
        # è®¡ç®—ç»Ÿè®¡é‡ï¼ˆåŸºäºæ»‘åŠ¨çª—å£ï¼‰
        rewards = [ep['total_reward'] for ep in self._progress_window]
        fidelities = [ep['fidelity'] for ep in self._progress_window]
        lengths = [ep['length'] for ep in self._progress_window]
        successes = [ep['success'] for ep in self._progress_window]
        
        progress_record = {
            'timesteps': self._total_timesteps,
            'mean_reward': float(np.mean(rewards)),
            'std_reward': float(np.std(rewards)),
            'mean_fidelity': float(np.mean(fidelities)),
            'std_fidelity': float(np.std(fidelities)),
            'success_rate': float(np.mean(successes)),
            'mean_length': float(np.mean(lengths))
        }
        
        # è¿½åŠ åˆ°CSVï¼ˆä½¿ç”¨é”é¿å…å¤šè¿›ç¨‹å†²çªï¼‰
        try:
            df = pd.DataFrame([progress_record])
            df.to_csv(self.progress_path, mode='a', header=False, index=False)
        except Exception as e:
            # é™é»˜å¤„ç†é”™è¯¯ï¼ˆå¯èƒ½æ˜¯å¤šè¿›ç¨‹å†™å…¥å†²çªï¼‰
            pass
    
    def _trigger_merge(self):
        """è§¦å‘CSVåˆå¹¶ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ç¯å¢ƒè¿è¡Œï¼‰"""
        try:
            # å¯¼å…¥åˆå¹¶å™¨
            import sys
            import os
            current_dir = os.path.dirname(os.path.abspath(__file__))
            parent_dir = os.path.dirname(current_dir)
            if parent_dir not in sys.path:
                sys.path.insert(0, parent_dir)
            
            from csv_merger import merge_csv_once
            
            # æ‰§è¡Œåˆå¹¶
            merge_csv_once(self.save_dir)
        except Exception as e:
            # æ‰“å°é”™è¯¯ä¾¿äºè°ƒè¯•ï¼ˆåªåœ¨å‰å‡ æ¬¡ï¼‰
            if hasattr(self, '_merge_error_count'):
                self._merge_error_count += 1
            else:
                self._merge_error_count = 1
            
            if self._merge_error_count <= 3:
                print(f"âš ï¸  CSVåˆå¹¶å¤±è´¥ (#{self._merge_error_count}): {e}")
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        # ğŸ”¥ ä¿å­˜å‰©ä½™çš„episodeæ•°æ®
        if self.enable_monitor and self.save_dir:
            self._save_episodes()
            self._trigger_merge()  # æœ€åä¸€æ¬¡åˆå¹¶
        
        if hasattr(self.env, 'close'):
            self.env.close()
    
    def seed(self, seed: int, dynamic_seed: bool = True):
        """è®¾ç½®éšæœºç§å­"""
        self._seed = seed
        self._dynamic_seed = dynamic_seed
        np.random.seed(seed)
        if hasattr(self.env, 'seed'):
            self.env.seed(seed)
        return [seed]
    
    def random_action(self):
        """è¿”å›ä¸€ä¸ªéšæœºåŠ¨ä½œ"""
        return self.action_space.sample()
    
    def __repr__(self):
        return f"<CircuitDesignerLightZeroEnv({self.env.name})>"
